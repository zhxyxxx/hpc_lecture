{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the `numpy.ndarray` internals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0, 1, 2, 3],[4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15]], dtype=np.int8)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 1. Understanding strides\n",
    "<mark>Question</mark> Determine the strides for the following arrays. Check your answer with `x.strides`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.reshape((2, 8))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2\n",
    "z = x.reshape((1, 16))\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3\n",
    "a = np.array([[0, 1, 2, 3],[4, 5, 6, 7],[8, 9, 10, 11], [12, 13, 14, 15]], dtype=np.int16)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2. Metadata modification vs copying the data buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>Question</mark> How do you explain the next result? Is it the same for `x.flatten()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(3, 3)\n",
    "y = x.ravel()  #  flatten the array\n",
    "y[0] = 100.\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>Question</mark> The next three cells do the same two operations: transposing a matrix and flattening it. How do you explain the difference in execution time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(5000, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# 2.1\n",
    "x.T\n",
    "x.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# 2.2\n",
    "x.T.ravel()   #### ADD A FIGURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# 2.3\n",
    "x.T\n",
    "x.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of broadcasting\n",
    "The concept [broadcasting](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) has to do with the way NumPy treats the arrays during operations involving different shapes. For instance, an array of shape `(5,)` added to an escalar, gives an array of shape `(5, )` where to all the elements was added the escaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(5)   # of shape (5,)\n",
    "x + 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important operation in broadcasting is to create new dimensions of an array using `np.newaxis` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(5)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:, np.newaxis].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[np.newaxis, :].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>Question</mark> From what you have already learned about the `numpy.ndarray`s, the operation `x[:, np.newaxis]` allocates new memory or can it be described with only a change on the metadata?\n",
    "\n",
    "***\n",
    "\n",
    "Broadcasting is often usefull to perform operations that are not vectorial in the mathematical sense, in a vectorial fashion. For instance, the next cell produces the array `y` with the different of all the possible combinations of the elements of `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x[:, np.newaxis] + x[np.newaxis, :]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here what happens is that each element of the `(5, 1)` array is added the `(5,)` element of the `(1, 5)` array. This will already know that produces an array of shape `(5,)`. Repeated for the five elements, this gives a `(5, 5)` array. \n",
    "\n",
    "***\n",
    "\n",
    "Let's see how to get the difference of all combinations of the `(3,)` elements of a `(10, 3)` array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(10, 3)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:, np.newaxis, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[np.newaxis, :, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x[np.newaxis, :, :] - x[:, np.newaxis, :]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Euclidean Distance Matrix with NumPy\n",
    "\n",
    "The Eucledian distance matrix is an $n \\times n$ matrix representing the spacing of a set of n points in Euclidean space. If $A$ is a Euclidean distance matrix and the points\n",
    "$\\mathbf{x}_{1}, \\mathbf{x}_{2}, \\ldots, \\mathbf{x}_{n}$\n",
    "are defined on $m$-dimensional space (each vector $\\mathbf{x}$ has length $m$), then the elements of $A$ are given by\n",
    "\\begin{align}\n",
    "A&=\\left(a_{i j}\\right);\\\\\n",
    "a_{i j}&=d_{i j}^{2}=\\left\\|\\mathbf{x}_{i}-\\mathbf{x}_{j}\\right\\|^{2}\n",
    "\\end{align}\n",
    "where $||.||^2$ denotes the 2-norm on $\\mathbb{R}^m$.\n",
    "\\begin{equation}\n",
    "A=\\left[\n",
    "\\begin{array}{ccccc}\n",
    "0 & d_{12}^{2} & d_{13}^{2} & \\dots & d_{1 n}^{2} \\\\\n",
    "d_{21}^{2} & 0 & d_{23}^{2} & \\dots & d_{2 n}^{2} \\\\\n",
    "d_{31}^{2} & d_{32}^{2} & 0 & \\dots & d_{3 n}^{2} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "d_{n 1}^{2} & d_{n 2}^{2} & d_{n 3}^{2} & \\dots & 0\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{equation}\n",
    "\n",
    "In this notebook we implement two functions to compute the Euclidean distance matrix. We use a simple algebra trick that makes possible to write the function in a completely vectorized way in terms of optimized NumPy functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Lets generate some random data\n",
    "npoints = 10\n",
    "ndimensions = 3\n",
    "x = 10. * np.random.random([npoints, ndimensions])\n",
    "y = 10. * np.random.random([npoints, ndimensions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Euclidean square distance matrix.\"\"\"\n",
    "diff = x[:, np.newaxis, :] - y[np.newaxis, :, :]\n",
    "dist = (diff * diff).sum(axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> Question </mark> At this point you are starting to get acquainted with the `numpy.ndarray`s and it's memory managment. Could you analyse the advantage and possible drawbacks of the `euclidean_broadcast` function?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider now a more sophisticated implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Euclidean trick\"\"\"\n",
    "x2 = np.einsum('ij,ij->i', x, x)[:, np.newaxis]\n",
    "y2 = np.einsum('ij,ij->i', y, y)[np.newaxis, :]\n",
    "xy = np.dot(x, y.T)\n",
    "dist = np.abs(x2 + y2 - 2. * xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `euclidean_trick` function\n",
    "\n",
    "Each element of the Euclidean distance matrix is the scalar product of the diference between two rows fo the dataset. `euclidean_trick` takes advantage of this by doing the following\n",
    "$$\\sum_k {(x_{ik}-y_{ik})^2} = (\\vec{x}_i - \\vec{y}_j)\\cdot(\\vec{x}_i - \\vec{y}_j) = \\vec{x}_i\\cdot\\vec{x}_i + \\vec{y}_j\\cdot\\vec{y}_j - 2\\vec{x}_i\\cdot\\vec{y}_j$$\n",
    "\n",
    "There are NumPy functions to compute each of these terms:\n",
    "\n",
    "$\\vec{x}_i\\cdot\\vec{y}_j$ $\\rightarrow$ `np.dot(x, y)` : Matrix product of $\\{\\vec{x}\\}$ and $\\{\\vec{y}\\}$\n",
    "\n",
    "$\\vec{x}_i\\cdot\\vec{x}_i$ $\\rightarrow$ `np.einsum('ij,ij->i', x, x)[:, np.newaxis]` : A $(n,1)$ vector of elements $\\sum_j x_{ij}x_{ij}$\n",
    "\n",
    "$\\vec{y}_j\\cdot\\vec{y}_j$ $\\rightarrow$ `np.einsum('ij,ij->i', y, y)[np.newaxis, :]` : A $(1,n)$ vector of elements $\\sum_j y_{ij}y_{ij}$\n",
    "\n",
    "To have all the combinations $ij$ of the sum $\\vec{x}_i\\cdot\\vec{x}_i + \\vec{y}_j\\cdot\\vec{y}_j$, we add a new axis to each of the arrays, transpose one them and add them.\n",
    "\n",
    "Let's see now how the `np.einsum` function works. `einsum` stands for Einstein summation, which is used in tensor algebra to write compact expressions without the sum symbol ($\\sum$). Within the Einstein summation notation, whenever there are repeated indexes, there is a sum over them. For instance, the expression\n",
    "$$x_{ik}y_{kj}$$\n",
    "is equivalent to\n",
    "$$\\sum_k x_{ik}y_{kj}$$\n",
    "\n",
    "`np.einsum` uses a generalized form of the Einstein summation by adding the symbol `->` to prevent summing over certain indexes. The specific operation we use here, `np.einsum('ij,ij->i', x, x)`, gives the vector\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\sum_k x_{1k}x_{1k} \\\\\n",
    "\\sum_k x_{2k}x_{2k} \\\\\n",
    " ...                \\\\\n",
    "\\sum_k x_{nk}x_{nk} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Note that the resulting vector is represented here as a column vector just for visualization purposes. It's is `(n,)` NumPy array.\n",
    "\n",
    "Let's check now step-by-step what the `euclidean_trick` function does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets generate some random data\n",
    "nsamples = 10\n",
    "nfeat = 3\n",
    "\n",
    "x = 10. * np.random.random([nsamples, nfeat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = np.einsum('ij,ij->i', x, x)\n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = np.einsum('ij,ij->i', x, x)[:, np.newaxis]\n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x2 + x2.T).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use `np.dot` to perform the matrix multiplication of the full dataset by itself. We didn't use it before as alternative to `np.einsum` because it doesn't perform row by row scalar products. Instead `np.dot` expects two arrays with matching shapes $(m,n)$ and $(n,m)$ to perform a matrix multiplication.\n",
    "\n",
    "We could have used `np.einsum('ik,jk', x, x)` to perform the matrix multiplication, but we chose `np.dot(x, x.T)` instead. This is because `np.dot` is a very sophisticated too, plus it uses OpenMP threads. This results in a very fast execution.\n",
    "\n",
    "You are wellcome to time them and look at the `top` command to see how `np.dot` uses multiple OpenMP threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.dot(x, x.T)\n",
    "xy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, considering that the reason we are using `np.einsum` is to get rid of the loops, why didn't we use something like `(x*x).sum(axis=1)`? Let's run the next cell comparing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use a larger array for timing the function calls\n",
    "nsamples = 1000\n",
    "nfeat = 300\n",
    "\n",
    "x = 10. * np.random.random([nsamples, nfeat])\n",
    "\n",
    "# it gives the same result\n",
    "np.abs(np.einsum('ij,ij->i', x, x) - (x*x).sum(axis=1)).max()\n",
    "\n",
    "# but it's not as fast as `np.einsum`\n",
    "%timeit np.einsum('ij,ij->i', x, x)\n",
    "%timeit (x*x).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing a reduction with the ufunc `np.add` is also slower than `np.einsum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np.add.reduce(x*x, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's time both implementations and check that they give the same result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_broadcast(x, y):\n",
    "    diff = x[:, np.newaxis, :] - y[np.newaxis, :, :]\n",
    "    return (diff * diff).sum(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_trick(x, y):\n",
    "    x2 = np.einsum('ij,ij->i', x, x)[:, np.newaxis]\n",
    "    y2 = np.einsum('ij,ij->i', y, y)[np.newaxis, :]\n",
    "    xy = np.dot(x, y.T)\n",
    "    return np.abs(x2 + y2 - 2. * xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 2000\n",
    "nfeat = 50\n",
    "\n",
    "x = 10. * np.random.random([nsamples, nfeat])\n",
    "\n",
    "%timeit euclidean_broadcast(x, x)\n",
    "%timeit euclidean_trick(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(euclidean_broadcast(x, x) - euclidean_trick(x, x)).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> Question </mark> Change the implementation of `euclidean_broadcast` function to make faster using `einsum` to do the final sum. How much is the speed up? Compare it with both the original `euclidean_broadcast` and `euclidean_trick`. Check that the result is the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "The main points to take from this notebook are:\n",
    "  * NumPy is all about vectorization. Loops in python must be avoided.\n",
    "  * Always consider different vectorized implementations and compare them.\n",
    "  * Even within NumPy, some functions might bring a more significant speedup than others."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
